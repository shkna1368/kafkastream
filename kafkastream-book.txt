we learned that Kafka has the
concept of named streams called topics. Furthermore, Kafka topics are extremely
flexible with what you store in them. For example, you can have homogeneous topics
that contain only one type of data, or heterogeneous topics that contain multiple types
of data.

So if we want to achieve some level of parallelism with the way we distribute
and process logs, we need to create lots of them. This is why Kafka topics are broken
into smaller units called partitions.

An event is a
timestamped key-value pair that records something that happened. The basic anatomy
of each event captured in a topic partition is:Headers-key-timestamps-value.

It also means that the communication backbone needs to be
scalable, i.e., able to handle increased amounts of load. This is why Kafka operates as
a cluster, and multiple machines, called brokers, are involved in the storage and
retrieval of data.

Consumer groups are made up of multiple cooperating consumers, and the membership
of these groups can change over time. For example, new consumers can come
online to scale the processing load, and consumers can also go offline either for planned
maintenance or due to unexpected failure. Therefore, Kafka needs some way of
maintaining the membership of each group, and redistributing work when necessary.

----------section 2 kafka streams
Kafka Streams is a lightweight, yet powerful Java library for enriching, transforming,
and processing real-time streams of data.

APIs for moving data to and from Kafka:Producer API-Consumer API-Connect API

Producer API:Writing messages to Kafka topics.Example:Filebeat-rsyslog-Custom producers.

- Consumer API:Reading messages from Kafka topics.Example:Logstash-kafkacat-custome consumer

-Connect API:Connecting external data stores, APIs, and filesystems to Kafka topics.Involves both reading from topics (sink connectors) and writing to topics (source connectors).Example:JDBC source connector-Elasticsearch sink connector-Custom connectors

moving data through Kafka is certainly important for creating data
pipelines, some business problems require us to also process and react to data as it
becomes available in Kafka. This is referred to as stream processing.

Kafka Streams is dedicated to
helping you process real-time data streams, not just move data to and from Kafka.6
It makes it easy to consume real-time streams of events as they move through our
data pipeline, apply data transformation logic using a rich set of stream processing
operators and primitives, and optionally write new representations of the data back
to Kafka.

Processing Model:Micro-batching involves grouping records into small batches and emitting
them to downstream processors at a fixed interval; event-at-a-time processing allows
each event to be processed at soon as it comes in, instead of waiting for a batch to
materialize.

Use Cases
Kafka Streams is optimized for processing unbounded datasets quickly and efficiently,
and is therefore a great solution for problems in low-latency, time-critical
domains.
The list goes on and on, but the common characteristic across all of these examples is
that they require (or at least benefit from) real-time decision making or data processing.

There are three basic kinds of processors in Kafka Streams:Source processors-Stream processors-Sink processors
Source processors
Sources are where information flows into the Kafka Streams application. Data is
read from a Kafka topic and sent to one or more stream processors.
Stream processors
These processors are responsible for applying data processing/transformation
logic on the input stream. In the high-level DSL, these processors are defined
using a set of built-in operators that are exposed by the Kafka Streams library,
which we will be going over in detail in the following chapters. Some example
operators are filter, map, flatMap, and join.

Sink processors
Sinks are where enriched, transformed, filtered, or otherwise processed records
are written back to Kafka, either to be handled by another stream processing
application or to be sent to a downstream data store via something like Kafka
Connect. Like source processors, sink processors are connected to a Kafka topic.

A collection of processors forms a processor topology, which is often referred to as
simply the topology in both this book and the wider Kafka Streams community. As we
go through each tutorial in this part of the book, we will first design the topology by
creating a DAG that connects the source, stream, and sink processors. Then, we will
simply implement the topology by writing some Java code.

if our application needs to consume from multiple source topics,
then Kafka Streams will (under most circumstances15) divide our topology into
smaller sub-topologies to parallelize the work even further. This division of work is
possible since operations on one input stream can be executed independently of operations
on another input stream.

Depth-First Processing
Kafka Streams uses a depth-first strategy when processing data. When a new record is
received, it is routed through each stream processor in the topology before another
record is processed.

A task is the smallest unit of work that can be performed in parallel in a Kafka Streams
application…

Slightly simplified, the maximum parallelism at which your application may run is
bounded by the maximum number of stream tasks, which itself is determined by the
maximum number of partitions of the input topic(s) the application is reading from.

High-Level DSL Versus Low-Level Processor API:
If you would like to build your stream processing application
using a functional style of programming, and would also like to leverage some
higher-level abstractions for working with your data (streams and tables), then the
DSL is for you.

On the other hand, if you need lower-level access to your data (e.g., access to record
metadata), the ability to schedule periodic functions, more granular access to your
application state, or more fine-grained control over the timing of certain operations,
then the Processor API is a better choice.

There are two
ways to model the data in your Kafka topics: as a stream (also called a record stream)
or a table (also known as a changelog stream).

Streams
These can be thought of as inserts in database parlance. Each distinct record
remains in this view of the log

Tables
Tables can be thought of as updates to a database. In this view of the logs, only
the current state (either the latest record for a given key or some kind of aggregation)
for each key is retained.

The following list includes a high-level overview of each:
KStream
A KStream is an abstraction of a partitioned record stream, in which data is represented
using insert semantics (i.e., each event is considered to be independent of
other events).
KTable
A KTable is an abstraction of a partitioned table (i.e., changelog stream), in which
data is represented using update semantics (the latest representation of a given
key is tracked by the application). Since KTables are partitioned, each Kafka
Streams task contains only a subset of the full table.28
GlobalKTable
This is similar to a KTable, except each GlobalKTable contains a complete (i.e.,
unpartitioned) copy of the underlying data. We’ll learn when to use KTables and
when to use GlobalKTables in Chapter 4.
The simplest form of stream processing requires no memory of previously seen
events. Each event is consumed, processed,1 and subsequently forgotten. This paradigm
is called stateless processing.
In stateless applications, each event handled by your Kafka Streams application is
processed independently of other events, and only stream views are needed by
your application (see “Streams and Tables” on page 53). In other words, your
application treats each event as a self-contained insert and requires no memory
of previously seen events.

• In stateless applications, each event handled by your Kafka Streams application is
processed independently of other events, and only stream views are needed by
your application (see “Streams and Tables” on page 53). In other words, your
application treats each event as a self-contained insert and requires no memory
of previously seen events.
• Stateful applications, on the other hand, need to remember information about
previously seen events in one or more steps of your processor topology, usually for
the purpose of aggregating, windowing, or joining event streams. These applications
are more complex under the hood since they need to track additional data,
or state.

In Kafka Streams, serializer and deserializer classes are often combined into a single
class called a Serdes,


Branching Data:Branching is typically required when events need to be routed to different
stream processing steps or output topics based on some attribute of the event
itself.

Let’s take a deeper look at Avro and learn how to create
an Avro data class for representing the enriched records in our Kafka Streams
application.

we need to convert a single
input record into a variable number of output records. Luckily for us, Kafka Streams
includes two operators that can help with this use case:
• flatMap
• flatMapValues


Avro is a popular format in the Kafka community, largely due to its compact byte representation
(which is advantageous for high throughput applications), native support
for record schemas,14 and a schema management tool called Schema Registry, which
works well with Kafka Streams and has had strong support for Avro since its inception.
15 There are other advantages as well. For example, some Kafka Connectors can
use Avro schemas to automatically infer the table structure of downstream data
stores, so encoding our output records in this format can help with data integration
downstream.


• Filtering data with filter and filterNot
• Creating substreams using the branch operator
• Combining streams with the merge operator
• Performing 1:1 record transformations using map and mapValues
• Performing 1:N record transformations using flatMap and flatMapValues
• Writing records to output topics using to, through, and repartition
• Serializing, deserializing, and reserializing data using custom serializers, deserializers,
and Serdes implementations

-----------statefull
Stateful processing helps us understand the relationships between events and leverage
these relationships for more advanced stream processing use cases. When we are able
to understand how an event relates to other events, we can:
• Recognize patterns and behaviors in our event streams
• Perform aggregations
• Enrich data in more sophisticated ways using joins.


Stateful Operators:Joining data-Aggregating data-Windowing data.

To support stateful operations, we need a way of storing and retrieving the remembered
data, or state, required by each stateful operator in our application (e.g., count,
aggregate, join, etc.). The storage abstraction that addresses these needs in Kafka
Streams is called a state store, and since a single Kafka Streams application can leverage
many stateful operators, a single application may contain several state stores.

Therefore, my recommendation is to start with persistent
stores and only switch to in-memory stores if you have measured a noticeable
performance improvement and, when quick recovery is concerned (e.g., in the event
your application state is lost), you are using standby replicas to reduce recovery time.

Before we start mapping our topics
to Kafka Streams abstractions, let’s first review the difference between KStream, KTa
ble, and GlobalKTable representations of a Kafka topic.

One important thing to look at when deciding between using a KTable or GlobalKTa
ble is the keyspace. If the keyspace is very large (i.e., has high cardinality/lots of
unique keys), or is expected to grow into a very large keyspace, then it makes more
sense to use a KTable.Perhaps a more important consideration when choosing between a KTable or Global
KTable is whether or not you need time synchronized processing. A KTable is time
synchronized, so when Kafka Streams is reading from multiple sources (e.g., in the
case of a join), it will look at the timestamp to determine which record to process
next.

These two characteristics (small and static data) are what GlobalKTables were
designed for. Therefore, we will use a GlobalKTable for our products topic.

selectKey is used to rekey records. In this case, it helps us meet the first copartitioning
requirement of ensuring records on both sides of the join (the
score-events data and players data) are keyed by the same field.
Grouping Streams:
• groupBy
• groupByKey

Aggregations:
• aggregate
• reduce
• count
